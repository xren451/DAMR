{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5de361f",
   "metadata": {},
   "source": [
    "# DAMR\n",
    "Hi, here is the code of DAMR(Dynamic Ajacency Matrix Representation) \n",
    "for data imputation in COVID-19 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9457a",
   "metadata": {},
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0eaaf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l2\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#from my own library import Generate_missing and other models\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \u001b[38;5;66;03m#generate_miss(data,perc),output:data after generate missing.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mFancyImpute\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFancyImpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "#1.Import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.special as special\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "import os\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, GRU, Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, LeakyReLU, GRU, Concatenate, Reshape, Softmax, Attention\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model\n",
    "#from keras.layers import LeakyReLU\n",
    "from spektral.layers import  GCSConv, DiffusionConv, GATConv, ARMAConv, GCNConv\n",
    "from tensorflow.keras.regularizers import l2\n",
    "#from my own library import Generate_missing and other models\n",
    "from model import * #generate_miss(data,perc),output:data after generate missing.\n",
    "import FancyImpute\n",
    "from FancyImpute import *\n",
    "import torch\n",
    "#Bidirectional LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bec6e",
   "metadata": {},
   "source": [
    "# 2. Data Pre-processing of COVID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameter\n",
    "Miss_perc=0.7\n",
    "Split_perc1=0.8\n",
    "Split_perc2=0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Missing 3D.\n",
    "path='Data/COVID/raw'\n",
    "COVID_features=Csv2Tensor(path)\n",
    "COVID_features=COVID_features[:503,:,:]\n",
    "COVID_features=MinMax3D(COVID_features)#(503,3,7)\n",
    "COVID_features_train=COVID_features[:int(Split_perc1*COVID_features.shape[0]),:,:]\n",
    "print(COVID_features_train.shape)#(402, 3, 7)\n",
    "COVID_features_val=COVID_features[int(Split_perc1*COVID_features.shape[0]):int(Split_perc1*COVID_features.shape[0])+int(Split_perc2*COVID_features.shape[0]),:,:]\n",
    "print(COVID_features_val.shape)#(50, 3, 7)\n",
    "COVID_features_test=COVID_features[int(Split_perc1*COVID_features.shape[0])+int(Split_perc2*COVID_features.shape[0]):,:,:]\n",
    "print(COVID_features_test.shape)#(51, 3, 7)\n",
    "\n",
    "#The mask and missing data has been created with a given miss percentage.\n",
    "#It will be used for baseline and for DAMR\n",
    "COVID_features_train_miss,COVID_features_train_mask=generate_miss_3D(COVID_features_train,Miss_perc)\n",
    "COVID_features_val_miss,COVID_features_val_mask=generate_miss_3D(COVID_features_val,Miss_perc)\n",
    "COVID_features_test_miss,COVID_features_test_mask=generate_miss_3D(COVID_features_test,Miss_perc)\n",
    "COVID_features_miss,COVID_features_mask=generate_miss_3D(COVID_features,Miss_perc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db251d09",
   "metadata": {},
   "source": [
    "# Run Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b9a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Lags for variable 1 ,modify input variable here!\n",
    "COVID_X1_all=COVID_features[:,0,:]\n",
    "#Transform the shape into Bidirectional problems\n",
    "COVID_X1_all=series_to_supervised(COVID_X1_all,15,16)\n",
    "COVID_Y1_all=COVID_X1_all.iloc[:,15*7:15*7+7]\n",
    "COVID_X1_all_1=COVID_X1_all.iloc[:,:15*7]\n",
    "COVID_X1_all_2=COVID_X1_all.iloc[:,15*7+7:]\n",
    "COVID_X1_all=np.hstack((COVID_X1_all_1,COVID_X1_all_2))\n",
    "COVID_X1_all=np.array(COVID_X1_all)\n",
    "COVID_X1_all=np.reshape(COVID_X1_all,(len(COVID_X1_all),COVID_features.shape[2],30))\n",
    "#Input 1->Groundtruth\n",
    "COVID_features=COVID_features[15:-15,:,:]#(473, 3, 7)\n",
    "print(COVID_features.shape)\n",
    "#Input2->Groundtruth\n",
    "print(COVID_X1_all.shape)#(473,7,30)\n",
    "COVID_Y1_all=np.array(COVID_Y1_all)\n",
    "COVID_Y1_all=np.reshape(COVID_Y1_all,(len(COVID_Y1_all),7,1))\n",
    "#Input3->Groundtruth\n",
    "Adj_all = np.arange(473*7*7).reshape(473,7,7)#(473,7,7)\n",
    "#Output1->Groundtruth\n",
    "print(COVID_Y1_all.shape)#(473, 7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Input and output in DAMR!\n",
    "# print(COVID_features_aftslid.shape)#473*3*7\n",
    "# print(COVID_X1_all_aftslid.shape)#473*7*30\n",
    "# print(COVID_Y1_all.shape)#(473, 7, 1)\n",
    "# print(Adj_dist.shape)#7*7\n",
    "\n",
    "#The mask and missing data has been created with a given miss percentage.\n",
    "#It will be used for baseline and for DAMR\n",
    "#Generate Missing 3D.\n",
    "COVID_features=Csv2Tensor(path)\n",
    "COVID_features=COVID_features[:503,:,:]\n",
    "COVID_features=MinMax3D(COVID_features)#(503,3,7)->Groundtruth\n",
    "COVID_features_train=COVID_features[:int(Split_perc1*COVID_features.shape[0]),:,:]\n",
    "print(COVID_features_train.shape)#(402, 3, 7)\n",
    "COVID_features_val=COVID_features[int(Split_perc1*COVID_features.shape[0]):int(Split_perc1*COVID_features.shape[0])+int(Split_perc2*COVID_features.shape[0]),:,:]\n",
    "print(COVID_features_val.shape)#(50, 3, 7)\n",
    "COVID_features_test=COVID_features[int(Split_perc1*COVID_features.shape[0])+int(Split_perc2*COVID_features.shape[0]):,:,:]\n",
    "print(COVID_features_test.shape)#(51, 3, 7)\n",
    "\n",
    "#\n",
    "COVID_features_train_miss,COVID_features_train_mask=generate_miss_3D(COVID_features_train,Miss_perc)#(402, 3, 7)\n",
    "COVID_features_val_miss,COVID_features_val_mask=generate_miss_3D(COVID_features_val,Miss_perc)#(50, 3, 7)\n",
    "COVID_features_test_miss,COVID_features_test_mask=generate_miss_3D(COVID_features_test,Miss_perc)#(51, 3, 7)\n",
    "COVID_features_miss,COVID_features_mask=generate_miss_3D(COVID_features,Miss_perc)\n",
    "\n",
    "Sliding_RMSE,Sliding_MAE,Sliding_MAPE,Sliding_MRE,Sliding_out=Sliding_impute_3D(COVID_features_miss,COVID_features,COVID_features_mask,6)#(503,3,7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lags for the first variable.\n",
    "#Extract the first variable\n",
    "lags=30\n",
    "var=0\n",
    "Sliding_out_var1=Sliding_out[:,var,:]\n",
    "Sliding_out_var1_sup=series_to_supervised(Sliding_out_var1,lags,1)\n",
    "print(Sliding_out_var1_sup.shape)\n",
    "Sliding_out_var1_sup=Sliding_out_var1_sup.iloc[:,:lags*Sliding_out.shape[2]]\n",
    "print(Sliding_out_var1_sup.shape)\n",
    "#Input1: 473*7*30\n",
    "COVID_X1_all_aftslid=np.array(Sliding_out_var1_sup).reshape(len(Sliding_out_var1_sup),COVID_features.shape[2],lags)\n",
    "#Input2: 473*3*7\n",
    "COVID_features_aftslid=Sliding_out[30:,:,:]\n",
    "#Input3: Adj: 473*7*7(Initialization)\n",
    "Adj_all = np.arange(len(COVID_features_aftslid)*COVID_features_aftslid.shape[2]*COVID_features_aftslid.shape[2]).reshape(len(COVID_features_aftslid),COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2])#(473,7,7)\n",
    "#Output:Groundtruth of the first variable->\n",
    "COVID_Y1_all=COVID_features[30:,:,:]\n",
    "COVID_Y1_all=COVID_Y1_all[:,var,:]\n",
    "COVID_Y1_all=COVID_Y1_all.reshape(COVID_Y1_all.shape[0],COVID_Y1_all.shape[1],1)\n",
    "#\n",
    "print(COVID_X1_all_aftslid.shape,COVID_features_aftslid.shape,Adj_all.shape,COVID_Y1_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Sliding_out_var1_sup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee302a2",
   "metadata": {},
   "source": [
    "# 3.Dynamic Adjacency matrix layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0894cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3.1 Distance-based adjacency matrix \n",
    "#Country: BZ CL ID IR MX PR SA.\n",
    "COVID_lat=pd.DataFrame([-14.235,-35.6751,20.5937,32.4279,23.6345,-9.19,30.5595])\n",
    "COVID_long=pd.DataFrame([-51.9253,-71.543,78.9629,53.688,-102.5528,-75.0152,22.9375])\n",
    "Adj_dist=adj_dist(COVID_lat,COVID_long)#7*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf855f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1=pd.DataFrame([[-51.9253,-71.543,78.9629],[53.688,-102.5528,-75.0152]])\n",
    "data2=pd.DataFrame([[-31.9253,-41.543,38.9629],[43.688,62.5528,65.0152]])\n",
    "Global_MeanMI(data1,data2,2)\n",
    "data1=pd.DataFrame(data1)\n",
    "data2=pd.DataFrame(data2)\n",
    "data1=np.array(data1).reshape(-1)\n",
    "data2=np.array(data2).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 Mean MI-based adjacency matrix\n",
    "print(COVID_features_aftslid.shape)\n",
    "MeanMI=[]\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        MeanMI.append(Global_MeanMI(COVID_features_aftslid[:,:,i],\n",
    "                                    COVID_features_aftslid[:,:,j],2))\n",
    "print(np.array(MeanMI).shape)\n",
    "MeanMI=np.array(MeanMI).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2])\n",
    "print(MeanMI.shape)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled1 = scaler.fit_transform(MeanMI)\n",
    "MeanMI=scaled1#shape:7*7\n",
    "print(MeanMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1682baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3.3 CONCAT MI-based adjacency matrix\n",
    "print(COVID_features_aftslid.shape)\n",
    "ConcatMI=[]\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        ConcatMI.append(Global_MeanMI(COVID_features_aftslid[:,:,i],\n",
    "                                    COVID_features_aftslid[:,:,j],2))\n",
    "ConcatMI=np.array(ConcatMI).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2])\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled1 = scaler.fit_transform(ConcatMI)\n",
    "ConcatMI=scaled1#shape:7*7\n",
    "print(ConcatMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.4 Single Feature MI-based adjacency matrix\n",
    "COVID_features_aftslid.shape\n",
    "FeatureMI=[]\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        FeatureMI.append(Global_EachMI(COVID_features_aftslid[:,:,i],\n",
    "                                    COVID_features_aftslid[:,:,j],2))\n",
    "#For the first feature,if choose the second feature, modify here!!\n",
    "FeatureMI=np.array(FeatureMI)[:,2]\n",
    "FeatureMI=np.array(FeatureMI).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2])\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled1 = scaler.fit_transform(FeatureMI)\n",
    "FeatureMI=scaled1#shape:7*7\n",
    "#MeanMI,ConcatMI,FeatureMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc314957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 Mean MI-based dynamic adjacency matrix\n",
    "print(COVID_features_aftslid.shape)\n",
    "#Index all of the rows and columns\n",
    "DyMeanMI=[]\n",
    "for k in range(COVID_features_aftslid.shape[0]):\n",
    "    for i in range(COVID_features_aftslid.shape[2]):\n",
    "        for j in range(COVID_features_aftslid.shape[2]):\n",
    "            DyMeanMI.append(Global_MeanMI(COVID_features_aftslid[:k,:,i],\n",
    "                                    COVID_features_aftslid[:k,:,j],2))\n",
    "DyMeanMI=np.array(DyMeanMI).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[0])\n",
    "DyMeanMI=DyMeanMI.transpose(2, 0, 1)\n",
    "from fancyimpute import SimpleFill, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "for i in range(DyMeanMI.shape[0]):\n",
    "    MEAN_imputer = SimpleFill()\n",
    "    DyMeanMI[i,:,:]=MEAN_imputer.fit_transform(DyMeanMI[i,:,:])\n",
    "print(DyMeanMI)#Here DyMeanMI is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.6 Concat MI-based dynamic adjacency matrix\n",
    "#Index all of the rows and columns\n",
    "DyConcatMI=[]\n",
    "for k in range(COVID_features_aftslid.shape[0]):\n",
    "    for i in range(COVID_features_aftslid.shape[2]):\n",
    "        for j in range(COVID_features_aftslid.shape[2]):\n",
    "            DyConcatMI.append(Global_MeanMI(COVID_features_aftslid[:k,:,i],\n",
    "                                    COVID_features_aftslid[:k,:,j],2))\n",
    "DyConcatMI=np.array(DyConcatMI).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[0])\n",
    "DyConcatMI=DyConcatMI.transpose(2, 0, 1)\n",
    "from fancyimpute import SimpleFill, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "for i in range(DyConcatMI.shape[0]):\n",
    "    MEAN_imputer = SimpleFill()\n",
    "    DyConcatMI[i,:,:]=MEAN_imputer.fit_transform(DyConcatMI[i,:,:])\n",
    "print(DyConcatMI)#Here DyConcatMI is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72deb9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DyConcatMI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5474ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3.7 FFT-based sum batch's MI dynamic adjacency matrix\n",
    "print(COVID_features_aftslid.shape)\n",
    "T=[20,40,100];\n",
    "Term1=T[0]\n",
    "Term2=T[1]\n",
    "Term3=T[2]\n",
    "#Step1: Calculate Adj in each batch\n",
    "#(1) Term1\n",
    "a=[]\n",
    "from math import ceil#Eg:Ceil(2.2)=3\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        for k in np.arange(ceil(COVID_features_aftslid.shape[0]/Term1)):\n",
    "            a.append(Global_MeanMI(COVID_features_aftslid[k*Term1:k*Term1+Term1,:,i],COVID_features_aftslid[k*Term1:k*Term1+Term1,:,j],2))\n",
    "a=np.array(a).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],ceil(COVID_features_aftslid.shape[0]/Term1))#a:(7, 7, 23)\n",
    "a=a.transpose(2,0,1)\n",
    "print(a.shape)#(24,7,7)\n",
    "\n",
    "#(2) Term2\n",
    "b=[]\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        for k in np.arange(ceil(COVID_features_aftslid.shape[0]/Term2)):\n",
    "            b.append(Global_MeanMI(COVID_features_aftslid[k*Term2:k*Term2+Term2,:,i],COVID_features_aftslid[k*Term2:k*Term2+Term2,:,j],2))\n",
    "b=np.array(b).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],ceil(COVID_features_aftslid.shape[0]/Term2))#a:(7, 7, 23)\n",
    "print(b.shape)#(7, 7, 12)\n",
    "b=b.transpose(2,0,1)\n",
    "print(b.shape)#(12,7,7)\n",
    "\n",
    "#(3) Term3\n",
    "c=[]\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        for k in np.arange(ceil(COVID_features_aftslid.shape[0]/Term3)):\n",
    "            c.append(Global_MeanMI(COVID_features_aftslid[k*Term3:k*Term3+Term3,:,i],COVID_features_aftslid[k*Term3:k*Term3+Term3,:,j],2))\n",
    "c=np.array(c).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],ceil(COVID_features_aftslid.shape[0]/Term3))#a:(7, 7, 23)\n",
    "print(c.shape)#(7, 7, 5)\n",
    "c=c.transpose(2,0,1)\n",
    "print(c.shape)#(5,7,7)\n",
    "#Step2:Assign it to particular position:\n",
    "#Each timestamp from the same batch share the same Adj.\n",
    "#MeanMI\n",
    "data=[]\n",
    "for j in range(a.shape[0]):\n",
    "    for i in range(Term1):\n",
    "        data.append(a[j,:,:])\n",
    "data=np.array(data)\n",
    "test=data\n",
    "DyConcatMI1=test[:DyConcatMI.shape[0],:,:]\n",
    "print(DyConcatMI1.shape)#(473,7,7)\n",
    "\n",
    "data=[]\n",
    "for j in range(b.shape[0]):\n",
    "    for i in range(Term2):\n",
    "        data.append(b[j,:,:])\n",
    "data=np.array(data)\n",
    "test=data\n",
    "DyConcatMI2=test[:DyConcatMI.shape[0],:,:]\n",
    "print(DyConcatMI2.shape)#(473,7,7)\n",
    "\n",
    "data=[]\n",
    "for j in range(c.shape[0]):\n",
    "    for i in range(Term3):\n",
    "        data.append(c[j,:,:])\n",
    "data=np.array(data)\n",
    "test=data\n",
    "DyConcatMI3=test[:DyConcatMI.shape[0],:,:]\n",
    "print(DyConcatMI3.shape)#(473,7,7)\n",
    "\n",
    "# Step3: Add all of the adjacency matrices\n",
    "DyConcatFFTMI=[]\n",
    "DyConcatFFTMI=DyConcatMI1+DyConcatMI2+DyConcatMI3\n",
    "print(DyConcatFFTMI.shape)#(473,7,7)\n",
    "\n",
    "# Step4: Normalize adjacency matrices\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "for i in range(DyConcatFFTMI.shape[0]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    DyConcatFFTMI[i,:,:] = scaler.fit_transform(DyConcatFFTMI[i,:,:])\n",
    "print(DyConcatFFTMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.8 FFT-based MI-based dynamic adjacency matrix\n",
    "print(COVID_features_aftslid.shape)\n",
    "T=[20,40,100];\n",
    "Term1=T[0]\n",
    "Term2=T[1]\n",
    "Term3=T[2]\n",
    "\n",
    "#Step1: Calculate Adj in each batch\n",
    "\n",
    "#(1) Term1\n",
    "a=[]\n",
    "from math import ceil#Eg:Ceil(2.2)=3\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        for k in np.arange(ceil(COVID_features_aftslid.shape[0]/Term1)):\n",
    "            a.append(Global_MeanMI(COVID_features_aftslid[k*Term1:k*Term1+Term1,:,i],COVID_features_aftslid[k*Term1:k*Term1+Term1,:,j],2))\n",
    "a=np.array(a).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],ceil(COVID_features_aftslid.shape[0]/Term1))#a:(7, 7, 23)\n",
    "a=a.transpose(2,0,1)\n",
    "print(a.shape)#(24,7,7)\n",
    "\n",
    "#(2) Term2\n",
    "b=[]\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        for k in np.arange(ceil(COVID_features_aftslid.shape[0]/Term2)):\n",
    "            b.append(Global_MeanMI(COVID_features_aftslid[k*Term2:k*Term2+Term2,:,i],COVID_features_aftslid[k*Term2:k*Term2+Term2,:,j],2))\n",
    "b=np.array(b).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],ceil(COVID_features_aftslid.shape[0]/Term2))#a:(7, 7, 23)\n",
    "print(b.shape)#(7, 7, 12)\n",
    "b=b.transpose(2,0,1)\n",
    "print(b.shape)#(12,7,7)\n",
    "\n",
    "#(3) Term3\n",
    "c=[]\n",
    "for i in range(COVID_features_aftslid.shape[2]):\n",
    "    for j in range(COVID_features_aftslid.shape[2]):\n",
    "        for k in np.arange(ceil(COVID_features_aftslid.shape[0]/Term3)):\n",
    "            c.append(Global_MeanMI(COVID_features_aftslid[k*Term3:k*Term3+Term3,:,i],COVID_features_aftslid[k*Term3:k*Term3+Term3,:,j],2))\n",
    "c=np.array(c).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],ceil(COVID_features_aftslid.shape[0]/Term3))#a:(7, 7, 23)\n",
    "print(c.shape)#(7, 7, 5)\n",
    "c=c.transpose(2,0,1)\n",
    "print(c.shape)#(5,7,7)\n",
    "#Step2: Calculate MI's MI for periodic term 1\n",
    "#Term1\n",
    "temp=[]\n",
    "for j in range(a.shape[0]):\n",
    "    for i in range(a.shape[0]):\n",
    "        temp.append(Global_MeanMI(a[j,:,:],a[i,:,:],2))#The first point MI's MI \n",
    "temp=np.array(temp).reshape(a.shape[0],a.shape[0])#24*24\n",
    "\n",
    "#Step3: Weighted sum for the same periodic term\n",
    "index_all=np.arange(temp.shape[0]*temp.shape[0]).reshape(a.shape[0],a.shape[0])#Initialization of index_all,shape=24*24\n",
    "for i in range(temp.shape[0]):\n",
    "    arrIndex=temp[i,:].argsort()\n",
    "    temp[i,:]=temp[i,:][arrIndex[::-1]]\n",
    "    index_all[i,:]=arrIndex\n",
    "print(temp.shape)#24*24\n",
    "print(index_all.shape)#24*24\n",
    "\n",
    "all_term=5#Get the first 5 decomposition\n",
    "a_all=np.zeros((a.shape[0],COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2]))\n",
    "for i in range(a.shape[0]):\n",
    "    for u in range(all_term):\n",
    "        a_all[i,:,:]=a_all[i,:,:]+a[i,:,:]+index_all[i,u]*temp[i,u]\n",
    "print(a_all.shape)#(24,7,7)\n",
    "\n",
    "#Step2: Calculate MI's MI for periodic term 2\n",
    "#Term2\n",
    "temp=[]\n",
    "for j in range(b.shape[0]):\n",
    "    for i in range(b.shape[0]):\n",
    "        temp.append(Global_MeanMI(b[j,:,:],b[i,:,:],2))#The first point MI's MI \n",
    "temp=np.array(temp).reshape(b.shape[0],b.shape[0])#12*12\n",
    "\n",
    "#Step3: Weighted sum for the same periodic term\n",
    "index_all=np.arange(temp.shape[0]*temp.shape[0]).reshape(b.shape[0],b.shape[0])#Initialization of index_all,shape=24*24\n",
    "for i in range(temp.shape[0]):\n",
    "    arrIndex=temp[i,:].argsort()\n",
    "    temp[i,:]=temp[i,:][arrIndex[::-1]]\n",
    "    index_all[i,:]=arrIndex\n",
    "print(temp.shape)#12*12\n",
    "print(index_all.shape)#12*12\n",
    "\n",
    "all_term=5#Get the first 5 decomposition\n",
    "b_all=np.zeros((b.shape[0],COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2]))\n",
    "for i in range(b.shape[0]):\n",
    "    for u in range(all_term):\n",
    "        b_all[i,:,:]=b_all[i,:,:]+b[i,:,:]+index_all[i,u]*temp[i,u]\n",
    "print(b_all.shape)#(12,7,7)\n",
    "\n",
    "#Step2: Calculate MI's MI for periodic term 3\n",
    "#Term3\n",
    "temp=[]\n",
    "for j in range(c.shape[0]):\n",
    "    for i in range(c.shape[0]):\n",
    "        temp.append(Global_MeanMI(c[j,:,:],c[i,:,:],2))#The first point MI's MI \n",
    "temp=np.array(temp).reshape(c.shape[0],c.shape[0])#12*12\n",
    "\n",
    "#Step3: Weighted sum for the same periodic term\n",
    "index_all=np.arange(temp.shape[0]*temp.shape[0]).reshape(c.shape[0],c.shape[0])#Initialization of index_all,shape=24*24\n",
    "for i in range(temp.shape[0]):\n",
    "    arrIndex=temp[i,:].argsort()\n",
    "    temp[i,:]=temp[i,:][arrIndex[::-1]]\n",
    "    index_all[i,:]=arrIndex\n",
    "print(temp.shape)#5*5\n",
    "print(index_all.shape)#5*5\n",
    "\n",
    "all_term=5#Get the first 5 decomposition\n",
    "c_all=np.zeros((c.shape[0],COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2]))\n",
    "for i in range(c.shape[0]):\n",
    "    for u in range(all_term):\n",
    "        c_all[i,:,:]=c_all[i,:,:]+c[i,:,:]+index_all[i,u]*temp[i,u]\n",
    "print(c_all.shape)#(5,7,7)\n",
    "\n",
    "#Step4:Assign it to particular position:\n",
    "#Each timestamp from the same batch share the same Adj.\n",
    "#MeanMI\n",
    "data=[]\n",
    "for j in range(a_all.shape[0]):\n",
    "    for i in range(Term1):\n",
    "        data.append(a_all[j,:,:])\n",
    "data=np.array(data)\n",
    "test=data\n",
    "DyConcatMI1=test[:DyConcatMI.shape[0],:,:]\n",
    "print(DyConcatMI1.shape)#(473,7,7)\n",
    "\n",
    "data=[]\n",
    "for j in range(b_all.shape[0]):\n",
    "    for i in range(Term2):\n",
    "        data.append(b_all[j,:,:])\n",
    "data=np.array(data)\n",
    "test=data\n",
    "DyConcatMI2=test[:DyConcatMI.shape[0],:,:]\n",
    "print(DyConcatMI2.shape)#(473,7,7)\n",
    "\n",
    "data=[]\n",
    "for j in range(c_all.shape[0]):\n",
    "    for i in range(Term3):\n",
    "        data.append(c_all[j,:,:])\n",
    "data=np.array(data)\n",
    "test=data\n",
    "DyConcatMI3=test[:DyConcatMI.shape[0],:,:]\n",
    "print(DyConcatMI3.shape)#(473,7,7)\n",
    "\n",
    "# Step5: Add all of the adjacency matrices\n",
    "DyConcatFFTeachMI=DyConcatMI1+DyConcatMI2+DyConcatMI3\n",
    "print(DyConcatFFTeachMI.shape)#(473,7,7)\n",
    "\n",
    "# Step6: Normalize adjacency matrices\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "for i in range(DyConcatFFTeachMI.shape[0]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    DyConcatFFTeachMI[i,:,:] = scaler.fit_transform(DyConcatFFTeachMI[i,:,:])\n",
    "print(DyConcatFFTeachMI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0dae9",
   "metadata": {},
   "source": [
    "# Begin with DAMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8702a0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Begin with DAMR\n",
    "print(COVID_features_aftslid.shape)#473*3*7\n",
    "print(COVID_X1_all_aftslid.shape)#473*7*30\n",
    "print(COVID_Y1_all.shape)#(473, 7, 1)\n",
    "print(Adj_dist.shape)#7*7\n",
    "# Set Hyperpaprameters\n",
    "tf.keras.backend.clear_session()#clear all\n",
    "tf.autograph.set_verbosity(0)#reset to zero\n",
    "learning_rate = 0.0001\n",
    "batch_size =24\n",
    "epochs = 100\n",
    "seed = 42\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d262d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define Model---GAT\n",
    "inputs_ret = Input(shape=(COVID_X1_all_aftslid.shape[1], COVID_X1_all_aftslid.shape[2], ))#7*30=node*lags(Time to look back)\n",
    "#inputs_feat = Input(shape=(COVID_features_aftslid.shape[2], COVID_features_aftslid.shape[1], ))#7*6=Node*feature\n",
    "inputs_adj = Input(shape=(Adj_dist.shape[1],Adj_dist.shape[1], ))#7*7=node*node\n",
    "print(inputs_ret.shape)\n",
    "#print(inputs_feat.shape)\n",
    "print(inputs_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf36ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GATinput:1.input nodes*lags 2.adj:nodes*nodes\n",
    "#GAToutput:1.channels*att_heads2.Att_weights=nodes*heads*nodes\n",
    "GAT_output_P, Att_weights_P =  GATConv(16, attn_heads=4, concat_heads=True, dropout_rate=0.3, return_attn_coef=True, \n",
    "                                   activation='relu', use_bias=False)([inputs_ret, inputs_adj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29795e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Att_weights_P = tf.math.reduce_mean(Att_weights_P, axis=2)#reduce heads,it will be nodes*nodes\n",
    "print(Att_weights_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da1929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GCN_output_P =  GCNConv(16, dropout_rate=0.3, \n",
    "#                         activation='relu', use_bias=True)([inputs_feat, inputs_adj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82d796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# diff_out = DiffusionConv(16, K=6, activation='tanh')([inputs_feat, Att_weights_P])#diff_out=nodes*channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d845b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#con_out_P_2 = Concatenate(axis=-1)([GAT_output_P, GCN_output_P,diff_out])\n",
    "dense_P = Dense(8, activation='relu')(GAT_output_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d4c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_out = Dropout(0.2)(dense_P)\n",
    "outputs = Dense(1)(dense_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8c088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[inputs_ret, inputs_adj], outputs=outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=[\"mae\", \"mape\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 473*7*7 for static adjacency matrix\n",
    "#3.1 Adj_dist\n",
    "Adj_dist=adj_dist(COVID_lat,COVID_long)#7*7\n",
    "Adj_dist=pd.DataFrame(Adj_dist)\n",
    "Adj_all = np.arange(473*7*7).reshape(473,7,7)#(473,7,7)\n",
    "Adj_all[0,:,:]=pd.DataFrame(Adj_all[0,:,:])\n",
    "Adj_all[0,:,:]=Adj_dist\n",
    "data=[]\n",
    "for i in range(Adj_all.shape[0]):\n",
    "    data.append(Adj_dist)\n",
    "data=np.array(data)\n",
    "Adj_dist=data\n",
    "#3.2 MeanMI\n",
    "Adj_MeanMI=MeanMI\n",
    "Adj_all = np.arange(473*7*7).reshape(473,7,7)#(473,7,7)\n",
    "Adj_all[0,:,:]=pd.DataFrame(Adj_all[0,:,:])\n",
    "Adj_all[0,:,:]=MeanMI\n",
    "data=[]\n",
    "for i in range(Adj_all.shape[0]):\n",
    "    data.append(Adj_MeanMI)\n",
    "data=np.array(data)\n",
    "Adj_MeanMI=data\n",
    "#3.3 ConcatMI\n",
    "Adj_ConcatMI=ConcatMI\n",
    "Adj_all = np.arange(473*7*7).reshape(473,7,7)#(473,7,7)\n",
    "Adj_all[0,:,:]=pd.DataFrame(Adj_all[0,:,:])\n",
    "Adj_all[0,:,:]=Adj_ConcatMI\n",
    "data=[]\n",
    "for i in range(Adj_all.shape[0]):\n",
    "    data.append(Adj_ConcatMI)\n",
    "data=np.array(data)\n",
    "Adj_ConcatMI=data\n",
    "#3.4 FeatureMI\n",
    "Adj_FeatureMI=FeatureMI\n",
    "Adj_all = np.arange(473*7*7).reshape(473,7,7)#(473,7,7)\n",
    "Adj_all[0,:,:]=pd.DataFrame(Adj_all[0,:,:])\n",
    "Adj_all[0,:,:]=Adj_FeatureMI\n",
    "data=[]\n",
    "for i in range(Adj_all.shape[0]):\n",
    "    data.append(Adj_FeatureMI)\n",
    "data=np.array(data)\n",
    "Adj_FeatureMI=data\n",
    "# #3.9 ->MeanMI approach(3.1+3.2+3.5+3.8)\n",
    "# coe1=0.1\n",
    "# coe2=0.8\n",
    "# coe3=0.4\n",
    "# coe4=0.1\n",
    "# Adj_all_MeanMI=coe1*Adj_dist+coe2*Adj_MeanMI+coe3*DyMeanMI+coe4*DyConcatFFTeachMI\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# for i in range(Adj_all_MeanMI.shape[0]):\n",
    "#     Adj_all_MeanMI[i,:,:]=scaler.fit_transform(Adj_all_MeanMI[i,:,:])\n",
    "# #3.10 ->()ConcatMI approach\n",
    "# Adj_all_ConcatMI=coe1*Adj_dist+coe2*Adj_MeanMI+coe3*DyMeanMI+coe4*DyConcatFFTeachMI\n",
    "# for i in range(Adj_all_ConcatMI.shape[0]):\n",
    "#     Adj_all_ConcatMI[i,:,:]=scaler.fit_transform(Adj_all_ConcatMI[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DyMeanMI\n",
    "# #3.2 Mean MI-based dynamic adjacency matrix\n",
    "# print(COVID_features_aftslid.shape)#(473, 3, 7)\n",
    "# #Index all of the rows and columns\n",
    "# MeanMI=[]\n",
    "# for k in range(COVID_features_aftslid.shape[0]):\n",
    "#     for i in range(COVID_features_aftslid.shape[2]):\n",
    "#         for j in range(COVID_features_aftslid.shape[2]):\n",
    "#             MeanMI.append(Global_MeanMI(COVID_features_aftslid[:k,:,i],\n",
    "#                                     COVID_features_aftslid[:k,:,j],2))\n",
    "# DyMeanMI=np.array(MeanMI).reshape(COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[2],COVID_features_aftslid.shape[0])\n",
    "# DyMeanMI=DyMeanMI.transpose(2, 0, 1)\n",
    "# print(DyMeanMI.shape)#(473,7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7610d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(COVID_features_aftslid.shape)#473*3*7\n",
    "COVID_features_aftslid= np.transpose(COVID_features_aftslid,(0,2,1))#473*7*3\n",
    "print(COVID_X1_all_aftslid.shape)#473*7*30\n",
    "print(np.array(COVID_Y1_all).shape)#(473, 7, 1)\n",
    "print(np.array(Adj_MeanMI).shape)#473*7*7\n",
    "#Normalization\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled1 = scaler.fit_transform(COVID_Y1_all[:,:,0])\n",
    "COVID_Y1_all[:,:,0]=scaled1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14252718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()#clear all\n",
    "tf.autograph.set_verbosity(0)#reset to zero\n",
    "learning_rate = 0.0001\n",
    "batch_size =72\n",
    "epochs = 100\n",
    "seed = 42\n",
    "verbose = 1\n",
    "ChangeAdj=Adj_FeatureMI\n",
    "x_train=COVID_X1_all_aftslid[:int(len(COVID_X1_all_aftslid)*0.8),:,:]\n",
    "x_val=COVID_X1_all_aftslid[int(len(COVID_X1_all_aftslid)*0.8):int(len(COVID_X1_all_aftslid)*0.9),:,:]\n",
    "x_test=COVID_X1_all_aftslid[int(len(COVID_X1_all_aftslid)*0.9):,:,:]\n",
    "features_train=COVID_features_aftslid[:int(len(COVID_features_aftslid)*0.8),:,:]\n",
    "features_val=COVID_features_aftslid[int(len(COVID_features_aftslid)*0.8):int(len(COVID_features_aftslid)*0.9),:,:]\n",
    "features_test=COVID_features_aftslid[int(len(COVID_features_aftslid)*0.9):,:,:]\n",
    "adj_train=ChangeAdj[:int(len(ChangeAdj)*0.8),:,:]\n",
    "adj_val=ChangeAdj[int(len(ChangeAdj)*0.8):int(len(ChangeAdj)*0.9),:,:]\n",
    "adj_test=ChangeAdj[int(len(ChangeAdj)*0.9):,:,:]\n",
    "y_train=COVID_Y1_all[:int(len(COVID_Y1_all)*0.8),:,:]\n",
    "y_val=COVID_Y1_all[int(len(COVID_Y1_all)*0.8):int(len(COVID_Y1_all)*0.9),:,:]\n",
    "y_test=COVID_Y1_all[int(len(COVID_Y1_all)*0.9):,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06954217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(features_train.shape)\n",
    "print(adj_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(features_val.shape)\n",
    "print(adj_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee46e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "# Set Hyperpaprameters\n",
    "tf.keras.backend.clear_session()#clear all\n",
    "tf.autograph.set_verbosity(0)#reset to zero\n",
    "\n",
    "\n",
    "# hists = model.fit(x= [x_train, features_train, adj_train],\n",
    "#                   y= y_train, verbose=verbose, epochs=500,\n",
    "#                   batch_size=batch_size, #validation_split=0.1, \n",
    "#                   validation_data=([x_val, features_val, adj_val], y_val),\n",
    "#                   #test_data=([x_test, features_test, adj_test], y_test),\n",
    "#   )\n",
    "\n",
    "##If you want to train faster, try earlystopping.\n",
    "\n",
    "hists = model.fit(x= [x_train, adj_train],\n",
    "                  y= y_train, verbose=verbose, epochs=500,\n",
    "                  batch_size=batch_size, #validation_split=0.1, \n",
    "                  validation_data=([x_val, adj_val], y_val),\n",
    "                  #test_data=([x_test, features_test, adj_test], y_test),\n",
    "                    callbacks=[EarlyStopping(#monitor=\"val_mean_absolute_error\", \n",
    "                      monitor=\"val_loss\", patience=20, restore_best_weights=True)],\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cab51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_pred - y_true))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y_pred - y_true)))\n",
    "\n",
    "def mape(y_true, y_pred, threshold=0.1):\n",
    "    v = np.clip(np.abs(y_true), threshold, None)\n",
    "    diff = np.abs((y_true - y_pred) / v)\n",
    "    return np.mean(diff, axis=-1).mean()\n",
    "###Write the function of mre\n",
    "def mre(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_pred - y_true))/np.sum(np.abs(y_true))\n",
    "def get_metrics(y, yp):\n",
    "    return {\n",
    "\n",
    "        \"mae\": np.round(mae(y, yp), 4),\n",
    "        \"mape\": np.round(mape(y, yp),4),\n",
    "        \"mre\": np.round(mre(y, yp),4),\n",
    "        \"rmse\": np.round(rmse(y, yp), 4),\n",
    "        #\"MPE\": np.round(MPE(y, yp), 4),\n",
    "        #\"R2\": np.round(R_squared(y, yp),4)   \n",
    "    }\n",
    "\n",
    "# scaled1 = scaler.fit_transform(MeanMI)\n",
    "#Get the tensor of test set after masking it.\n",
    "test_predicted = model.predict([x_test, adj_test])\n",
    "get_metrics(y_test.flatten(), test_predicted.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadb62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the mask of y_test.\n",
    "Mask_BZ=pd.read_csv(\"Data/COVID/10%/BZ_10.csv\")\n",
    "Mask_CL=pd.read_csv(\"Data/COVID/10%/CL_10.csv\")\n",
    "Mask_ID=pd.read_csv(\"Data/COVID/10%/ID_10.csv\")\n",
    "Mask_IR=pd.read_csv(\"Data/COVID/10%/IR_10.csv\")\n",
    "Mask_MX=pd.read_csv(\"Data/COVID/10%/MX_10.csv\")\n",
    "Mask_PR=pd.read_csv(\"Data/COVID/10%/PR_10.csv\")\n",
    "Mask_SA=pd.read_csv(\"Data/COVID/10%/SA_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask_BZ=np.array(Mask_BZ)\n",
    "missing_mask=[]\n",
    "for i in range(Mask_BZ.shape[0]):\n",
    "    missing_mask.append(np.isnan(np.array(Mask_BZ[i])))\n",
    "missing_mask=np.array(missing_mask).reshape(Mask_BZ.shape[0],Mask_BZ.shape[1])\n",
    "missing_mask_BZ_test=missing_mask[-15-y_test.shape[0]:-15,:]#48*3\n",
    "#Get the first feature's mask of BZ\n",
    "missing_mask_BZ_test1=missing_mask_BZ_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask_CL=np.array(Mask_CL)\n",
    "missing_mask=[]\n",
    "for i in range(Mask_CL.shape[0]):\n",
    "    missing_mask.append(np.isnan(np.array(Mask_CL[i])))\n",
    "missing_mask=np.array(missing_mask).reshape(Mask_CL.shape[0],Mask_CL.shape[1])\n",
    "missing_mask_CL_test=missing_mask[-15-y_test.shape[0]:-15,:]#48*3\n",
    "#Get the first feature's mask of CL\n",
    "missing_mask_CL_test1=missing_mask_CL_test[:,0]\n",
    "Mask_ID=np.array(Mask_ID)\n",
    "missing_mask=[]\n",
    "for i in range(Mask_ID.shape[0]):\n",
    "    missing_mask.append(np.isnan(np.array(Mask_ID[i])))\n",
    "missing_mask=np.array(missing_mask).reshape(Mask_ID.shape[0],Mask_ID.shape[1])\n",
    "missing_mask_ID_test=missing_mask[-15-y_test.shape[0]:-15,:]#48*3\n",
    "#Get the first feature's mask of ID\n",
    "missing_mask_ID_test1=missing_mask_ID_test[:,0]\n",
    "Mask_IR=np.array(Mask_IR)\n",
    "missing_mask=[]\n",
    "for i in range(Mask_IR.shape[0]):\n",
    "    missing_mask.append(np.isnan(np.array(Mask_IR[i])))\n",
    "missing_mask=np.array(missing_mask).reshape(Mask_IR.shape[0],Mask_IR.shape[1])\n",
    "missing_mask_IR_test=missing_mask[-15-y_test.shape[0]:-15,:]#48*3\n",
    "#Get the first feature's mask of IR\n",
    "missing_mask_IR_test1=missing_mask_IR_test[:,0]\n",
    "Mask_MX=np.array(Mask_MX)\n",
    "missing_mask=[]\n",
    "for i in range(Mask_MX.shape[0]):\n",
    "    missing_mask.append(np.isnan(np.array(Mask_MX[i])))\n",
    "missing_mask=np.array(missing_mask).reshape(Mask_MX.shape[0],Mask_MX.shape[1])\n",
    "missing_mask_MX_test=missing_mask[-15-y_test.shape[0]:-15,:]#48*3\n",
    "#Get the first feature's mask of MX\n",
    "missing_mask_MX_test1=missing_mask_MX_test[:,0]\n",
    "Mask_PR=np.array(Mask_PR)\n",
    "missing_mask=[]\n",
    "for i in range(Mask_PR.shape[0]):\n",
    "    missing_mask.append(np.isnan(np.array(Mask_PR[i])))\n",
    "missing_mask=np.array(missing_mask).reshape(Mask_PR.shape[0],Mask_PR.shape[1])\n",
    "missing_mask_PR_test=missing_mask[-15-y_test.shape[0]:-15,:]#48*3\n",
    "#Get the first feature's mask of PR\n",
    "missing_mask_PR_test1=missing_mask_PR_test[:,0]\n",
    "Mask_SA=np.array(Mask_SA)\n",
    "missing_mask=[]\n",
    "for i in range(Mask_SA.shape[0]):\n",
    "    missing_mask.append(np.isnan(np.array(Mask_SA[i])))\n",
    "missing_mask=np.array(missing_mask).reshape(Mask_SA.shape[0],Mask_SA.shape[1])\n",
    "missing_mask_SA_test=missing_mask[-15-y_test.shape[0]:-15,:]#48*3\n",
    "#Get the first feature's mask of SA\n",
    "missing_mask_SA_test1=missing_mask_SA_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32546cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mask_ALL_test1=np.stack((missing_mask_BZ_test1,missing_mask_CL_test1\n",
    "            ,missing_mask_ID_test1,missing_mask_IR_test1,\n",
    "            missing_mask_MX_test1,missing_mask_PR_test1,missing_mask_SA_test1\n",
    "           ),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get test set result.\n",
    "get_metrics(y_test[missing_mask_ALL_test1].flatten(), test_predicted[missing_mask_ALL_test1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29d042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
